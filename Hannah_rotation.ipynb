{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca689150-723d-4d7c-861c-81842cb23e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages to be used during analysis\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import matplotlib.patches as mpatches\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0451ff-0fda-4d6e-80dc-83304029711b",
   "metadata": {},
   "source": [
    "# Define Functions\n",
    "\n",
    "The below code block will define functions we will use during our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "44f099df-776a-4e40-94be-dfb04e263978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFile(path, headers=None):\n",
    "    '''\n",
    "    This function will parse a standard HEKA .asc file into a pandas dataframe.\n",
    "\n",
    "    Arguments: \n",
    "    path - a stringIO input of a standard HEKA output .asc file.\n",
    "\n",
    "    Returns:\n",
    "    df, dfcache - two copies of the file reformatted into a dataframe.\n",
    "    '''\n",
    "\n",
    "    lineIndices = []            \n",
    "    \n",
    "    # Splits string at \\n and removes trailing spaces  \n",
    "    with open(path, \"r\") as f:                        \n",
    "        rawFile = f.read().strip().split(\"\\n\")         \n",
    "\n",
    "    count=0\n",
    "    # Finds rows that contain header information to exclude from df                                     \n",
    "    for line in rawFile:                                  \n",
    "        if re.search(r\"[a-z]+\", line) == None:           \n",
    "            lineIndices.append(count)                     \n",
    "        count += 1                                    \n",
    "    \n",
    "    # Formats headerless file for later df\n",
    "    processedFile = [rawFile[i].strip().replace(\" \", \"\").split(\",\") for i in lineIndices]     \n",
    "\n",
    "    # Use the difference in file size with and without headers to find nSweeps\n",
    "\n",
    "    if headers == None:\n",
    "         df = pd.DataFrame(data=processedFile)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=headers, data=processedFile)\n",
    "    df = df.apply(pd.to_numeric)\n",
    "    df = df.dropna(axis=0)\n",
    "\n",
    "    nSweeps = df[\"index\"].tolist().count(0) \n",
    "\n",
    "    # Make new column with sweep identity\n",
    "    df['sweep'] = np.repeat(np.arange(nSweeps) + 1, len(df)/(df[\"index\"].tolist().count(1)))\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def preprocess(df):\n",
    "    df.i *= 1e12\n",
    "    df.v *= 1000\n",
    "    df.ti *= 1e3\n",
    "    df.p /= 0.02\n",
    "    \n",
    "    i_blsub = df.groupby(\"sweep\").apply(lambda g: g.i - g.query(\"ti < 1000 & ti > 500\").mean().i)\n",
    "    new_df = df.assign(i_blsub = np.array(i_blsub))\n",
    "    return new_df\n",
    "\n",
    "def generatePaths(folderPath=os.getcwd(), suffix=\".asc\"):\n",
    "    \"\"\"\n",
    "    This function will run the preprocess function on all files in a folder.\n",
    "    \"\"\"\n",
    "    path_list = []\n",
    "\n",
    "    for root, dirs, files in os.walk(folderPath):\n",
    "        for file in files:\n",
    "            if file.find(suffix) != -1:\n",
    "                path_list.append(os.path.join(root, file).replace(\"\\\\\",\"/\"))\n",
    "    \n",
    "    return path_list\n",
    "\n",
    "def plotSweeps(df, x, y, p_kwargs={}, i_kwargs={}):\n",
    "    f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, gridspec_kw={'height_ratios':[1,3]}, figsize=(20,10))\n",
    "    f.patch.set_facecolor('white')\n",
    "    \n",
    "    for k, g in df.groupby(\"sweep\"):\n",
    "        ax1.plot(g.ti, g.p, **p_kwargs)\n",
    "        ax1.set_ylabel(\"Pressure (mmHg)\")\n",
    "        ax1.spines['top'].set_visible(False)\n",
    "        ax1.spines['right'].set_visible(False)\n",
    "        ax1.spines['top'].set_visible(False)\n",
    "        ax1.spines['bottom'].set_visible(False)\n",
    "        ax1.axes.xaxis.set_visible(False)\n",
    "        ax2.plot(g[x], g[y], **i_kwargs)\n",
    "        ax2.set_xlabel(\"Time (ms)\")\n",
    "        ax2.set_ylabel(\"Current (pA)\")\n",
    "        ax2.spines['top'].set_visible(False)\n",
    "        ax2.spines['right'].set_visible(False)\n",
    "        \n",
    "    return ax1, ax2\n",
    "\n",
    "def findSmoothedPeaks(df, smoothing = 5, window=[5100,5400]):\n",
    "    \n",
    "    ## Error checking\n",
    "    if smoothing % 2 == 0:\n",
    "        raise EvenValueError(\"You assigned an even value to smoothing. The smoothing value must be an odd integer.\")\n",
    "    elif type(smoothing) == type(1.5):\n",
    "        raise NonIntegerValueError(\"You assigned a non-integer value to smoothing. The smoothing value must be an odd integer.\")\n",
    "            \n",
    "    gdf = df.groupby('sweep')\n",
    "    datStim = df.query(\"ti >= @window[0] & ti < @window[1]\")\n",
    "    peaks = datStim.groupby(\"sweep\").apply(lambda g: g.iloc[np.argmax(g.i_blsub)]).reset_index(drop=True)\n",
    "    npoints = (smoothing-1)/2\n",
    "    smoothed_peaks = np.zeros(len(peaks))\n",
    "    smoothed_peaks_sd = np.zeros(len(peaks))\n",
    "    \n",
    "    for i in range(len(peaks)):\n",
    "        ind = peaks.loc[i,'index']\n",
    "        smoothed_peaks[i] = gdf.get_group(i+1).query(\"index >= @ind - @npoints & index <= @ind + @npoints\").i_blsub.mean()\n",
    "        smoothed_peaks_sd[i] = gdf.get_group(i+1).query(\"index >= @ind - @npoints & index <= @ind + @npoints\").i_blsub.std()\n",
    "        \n",
    "    return pd.DataFrame({\"peak_time\":peaks.ti ,\"mean_peak\":smoothed_peaks,\"sd_peak\":smoothed_peaks_sd})\n",
    "\n",
    "def findSteadyState(df, window = [5395-20, 5395]):\n",
    "    \n",
    "    ## Error checking\n",
    "    if len(window) != 2:\n",
    "        raise EvenValueError(\"Window must be an iterable of length 2 with the start and end points of the region of interest.\")\n",
    "        \n",
    "    ss_mean = df.query(\"ti >= @window[0] & ti < @window[1]\").groupby(\"sweep\").i_blsub.apply(np.mean)\n",
    "    ss_sd = df.query(\"ti >= @window[0] & ti < @window[1]\").groupby(\"sweep\").i_blsub.apply(np.std)\n",
    "    \n",
    "    return pd.DataFrame({\"mean_ss\":ss_mean,\"sd_ss\":ss_sd}).reset_index(drop=True)\n",
    "\n",
    "def inactivationanalysis(df, smoothing = 5, spwindow = [5100, 5400], sswindow = [5395-20,5395]):\n",
    "    sp = findSmoothedPeaks(df, smoothing = smoothing, window = spwindow)\n",
    "    ss = findSteadyState(df, window = sswindow)\n",
    "    return pd.concat([sp, ss], axis=1)\n",
    "\n",
    "def analyzedtable(filelist):\n",
    "    for i in range(len(filelist)):\n",
    "    \n",
    "        temp = pd.read_csv(filelist[i], header=0, index_col=0)\n",
    "        filename = filelist[i].split(\"/\")[-1]\n",
    "    \n",
    "        if filename.find(\"PGK\") != -1:\n",
    "            promoter = \"PGK\"\n",
    "        elif filename.find(\"WT\") != -1:\n",
    "            promoter = \"WT\"\n",
    "        else:\n",
    "            promoter = \"CMV\"\n",
    "\n",
    "        if filename.find(\"pos\") != -1:\n",
    "            condition = \"pos\"\n",
    "        else:\n",
    "            condition = \"neg\"\n",
    "\n",
    "        temp = temp.assign(promoter = np.repeat(promoter, 5),\n",
    "                           filename = np.repeat(filename, 5),\n",
    "                           condition = np.repeat(condition, 5))\n",
    "\n",
    "        if i == 0:\n",
    "            agg = temp\n",
    "        else:\n",
    "            agg = pd.concat([agg, temp], axis=0)\n",
    "        \n",
    "    return agg\n",
    "    \n",
    "\n",
    "class Error(Exception):\n",
    "    \"\"\"Base class for other exceptions\"\"\"\n",
    "    pass\n",
    "\n",
    "class EvenValueError(Error):\n",
    "    \"\"\"Raised when an even number is passed to an argument that requires an odd input\"\"\"\n",
    "    pass\n",
    "\n",
    "class NonIntegerValueError(Error):\n",
    "    \"\"\"Raised when a non-integer number is passed to an argument that requires an integer input.\"\"\"\n",
    "    pass\n",
    "\n",
    "class ListSizeError(Error):\n",
    "    \"\"\"Raised when a non-integer number is passed to an argument that requires an integer input.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce1492a-0a30-43e0-8a0a-0b2b2da7b8f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load and Preprocess File\n",
    "\n",
    "We call the loadFile function to load a file into our workspace for analysis.\n",
    "\n",
    "Then, our preprocessing will consist of a few steps. First we will convert some of the units into something more convenient:\n",
    "- i: A -> pA\n",
    "- v: V -> mV\n",
    "- ti: s -> ms\n",
    "- p: v -> mmHg (200 mV / 10 mmHg)\n",
    "\n",
    "The general notation used for all these conversions are `df.x *= y`. This notation tells python to take the column `x` of the dataframe `df`, multiply each value in it by the constat `y` and set the dataframe column equal to the returned value. \n",
    "\n",
    "\n",
    "Next, we will baseline subtract the current and append this baseline subtracted data into a new column called `i_blsub`. Let's talk a bit about the lines of code that accomplish this as they use both numpy and pandas packages.\n",
    "\n",
    "## Split-Apply-Combine\n",
    "The split-apply-combine paradigm is one of the most important working principles and is broadly used in any sort of analysis. Oftentimes you want to split your data based on some factor, perform some operation on each group, then return either a summary table or a transformed dataset. Pandas is designed with this workflow in mind and makes it quite convenient notationwise.\n",
    "In our case what we want to do is as follows:\n",
    "1. Split the data by sweep number\n",
    "2. Apply a baseline subtraction based on the mean of the current in a particular time window (500 - 1000 ms)\n",
    "3. Transform i into a new column i_blsub that is the baseline subtracted current\n",
    "\n",
    "## Pandas chaining\n",
    "Most often, when we apply functions to an object and we nest those functions like f(g(x)) we work from the inside out. So we apply g(x) first then apply f(x) to the output of g(x). For example the function `print(mean(x))` will find the mean of x then print that value.\n",
    "\n",
    "Pandas allows for a different workflow called \"chaining\" or \"piping\". You'll find similar ideas in R when using the `dplyr` package. You can recognize this by the \".\" (dot) syntax. Anytime you see a variable or operation followed by a period that means the next command will be applied to the output of what is to the immediate left of that period. A simple example is `df.i` which is simply get column `i` of the dataframe `df`.\n",
    "\n",
    "## Lambda Functions\n",
    "\n",
    "Lambda functions are essentially quick one-off functions you may write. Typically, when you write functions you want to reuse you use the more typical convention of:\n",
    "\n",
    "```\n",
    "def myFunction(args):\n",
    "    var = dosomething(args)\n",
    "    return var\n",
    "```\n",
    "\n",
    "However, at times you just want something quick and for a single use. The general form of a lambda function is ax follows:\n",
    "\n",
    "```\n",
    "lambda x: dosomething(x)\n",
    "```\n",
    "\n",
    "The lambda function will return whatever the output of the operations performed on x is. Here, x is an anonymous variable so we can really call it anything.\n",
    "\n",
    "Now we have all the tools (pandas chaining to accomplish split-apply-combine and lambda functions) to breakdown the following line of code:\n",
    "\n",
    "`i_blsub = df.groupby(\"sweep\").apply(lambda g: g.i - g.query(\"ti < 1000 & ti > 500\").mean().i)`\n",
    "\n",
    "At first glance it may seem complicated but let's go step-by-step. \n",
    "\n",
    "1. We split the data into groups based on sweep number with df.groupby(\"sweep\"). Groupby can either take as arguments a single column name as a string or multiple column names as a list of strings. For instance, you can say `df.groupby([\"sweep\", \"cellID\"])` if we had a separate column of cellIDs and it will use both columns to form unique groups. In python `[]` denotes lists. \n",
    "\n",
    "2. We want to do something to each group. That is where the `apply` operation comes into play. The apply function takes a single argument, another function, that is to be applied to each group that is passed to it in the chaining operation. For instance, one way to find group means is to type `df.groupby(\"sweep\").apply(mean)` and it will find the mean of every group. \n",
    "\n",
    "3. However, we don't just want the group means. We want only the mean current within a certain region of the trace. Even better, we want to subtract this mean current from the raw current within each group. To do this we use a lambda function. Here, our lambda function is defined as `lambda g: g.i - g.query(\"ti < 1000 & ti > 500\").mean().i`. Notice the use of pandas chaining within the lambda function. As a result we can read what it is doing from left to right. We know based on the chaining we did earlier that whatever `apply` does will be applied groupwise so we call our variable `g` for group (for clarity). We want to subtract something from the current columnn in each group so we call that column using `g.i`. Now we need to deal with windowing and finding the mean. We know that each group `g` is essentially a mini dataframe so we can `query` that dataframe to window a particular part of it. Again, using chaining we can do this by typing `g.query()`. We can then tell it how to query our dataframe. We want the region where the time is between 500 and 1000 ms. This can be accomplished by writing `'ti < 1000 & ti > 500'`. I much prefer query to classical subsetting because it is optimized by the wonderful people that wrote pandas to be performant and it is very easy to read and figure out what is going on. Python is all about readability. `g.query(\"ti < 1000 & ti > 500\")` gives us the subset of the dataframe we are interested. Now we can find the mean of each column of the dataframe by chaining on the `mean` operation giving us `g.query(\"ti < 1000 & ti > 500\")`. At this stage we have the means of all the columns within this subset of the dataframe but we only care about the mean of the current so we can access this by chaining on a call to the column as follows: `g.query(\"ti < 1000 & ti > 500\").mean().i`. Great! Now our lambda function can output the raw current trace `g.i` minus the mean within this particular region we found with `g.query(\"ti < 1000 & ti > 500\").mean().i`.\n",
    "\n",
    "We save all of this to our new variable i_blsub but it would really be much more useful to add this as a new column to our original dataframe. This is relatively easy since we know that the column should be the same size (since we are simply transforming the column `df.i`. We handle this in the next line of code.\n",
    "\n",
    "`df = df.assign(i_blsub = np.array(i_blsub))`\n",
    "\n",
    "We transform the contents of our variable i_blsub into a numpy array with the function `np.array()`. This just avoids some errors that would come up based on the grouping of `i_blsub`. It will get rid of any grouping and just make it an array of numbers. Then we can update our dataframe `df` by setting it equal to the output of `df.assign()`. `df.assign()` works as follows:\n",
    "\n",
    "`df.assign(column_name = column_contents)`\n",
    "\n",
    "So, we will add a new column `i_blsub` and assign it the contents of the variable `i_blsub` after transforming it into a numpy array.\n",
    "\n",
    "`df = df.assign(i_blsub = np.array(i_blsub))`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc06a316-a862-4a49-93f4-4841a892f683",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = generatePaths()\n",
    "dat  = loadFile(files[0], headers= [\"index\", \"ti\", \"i\", \"tp\",\"p\",\"tv\",\"v\"])\n",
    "\n",
    "dat.i *= 1e12\n",
    "dat.v *= 1000\n",
    "dat.ti *= 1e3\n",
    "dat.p /= 0.02\n",
    "\n",
    "## Baseline subtract\n",
    "i_blsub = dat.groupby(\"sweep\").apply(lambda g: g.i - g.query(\"ti < 1000 & ti > 500\").mean().i)\n",
    "dat = dat.assign(i_blsub = i_blsub.reset_index(drop=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2820e4ea-c8aa-4e94-9147-0639e78aa95b",
   "metadata": {},
   "source": [
    "# Defining reusable functions\n",
    "\n",
    "We talked about functions a bit earlier when I mentioned lambda functions. Whenever you have an operation you know you will perform repeatedly it is a good idea to package the contents into a function. We developed a workflow for preprocessing our data for later analysis so now lets put this all into a function! Whenever you define a function in python the organization is as follows.\n",
    "\n",
    "``` \n",
    "def myfunction(args):\n",
    "    output = dosomething(args)\n",
    "    return output\n",
    "```\n",
    "\n",
    "It starts with `def` for define. This is followed by whatever you want to call your function (it should make sense). In the example case this is just called myfunction. Then we list any arguments which is a list of things we want to pass to the function. The simplest function may only have a single argument. Some functions have many. These arguments are passed to the function's scope. We need to end this initial definition with a colon (python convention) and any subsequent lines that are a part of the function should be indented. Then, we put any operations we want performed in the body of our function. Finally, we want to return the output of those operations back to the global scope. I used the word scope here a few times so let's talk about it.\n",
    "\n",
    "## Scope \n",
    "\n",
    "Scope is simply the space of variables that your function can see and access. All the variables we have defined so far have been in global scope. However, you can define variables within a function that can only be seen within the function unless explicitly returned to global scope. For instance, if we just type in python `apple = 4` then later call the variable `apple` it will return the value `4`. That's because we defined the variable in global scope. \n",
    "\n",
    "A variable can be defined within a function for temporary use within the scope of what that function needs to do. Let's take the example function:\n",
    "\n",
    "```\n",
    "def myfunction(x):\n",
    "    y = 3x\n",
    "    z = 4y + 2\n",
    "    return z\n",
    "```\n",
    "\n",
    "We can then have the function operate on some number and save it to a new variable as follows: `orange = myfunction(2)`. Now, we can call `orange` and get the output of the function which in this case is `26`. However, if we type `y` or `z` we get an error. This is because these variables are not in the global scope. They were only temporarily used by the function to generate the output.\n",
    "\n",
    "## Putting it all together\n",
    "\n",
    "Okay, we now have the tools to build our function. Let's call it preprocess. I typically like to name the arguments something informative. For instance df tells us that the argument is a dataframe. The function will convert units and perform background subtraction on an input dataframe then return a new, updated dataframe. The body can simply be the code we wrote earlier. Importantly, since our argument is called df, regardless of what the dataframe may be called in global space, within the function it is called df. So we will have to change the name in the function body accordingly.\n",
    "\n",
    "```\n",
    "def preprocess(df):\n",
    "    df.i *= 1e12\n",
    "    df.v *= 1000\n",
    "    df.ti *= 1e3\n",
    "    df.p /= 0.02\n",
    "    \n",
    "    i_blsub = df.groupby(\"sweep\").apply(lambda g: g.i - g.query(\"ti < 1000 & ti > 500\").mean().i)\n",
    "    new_df = df.assign(i_blsub = np.array(i_blsub))\n",
    "    return new_df\n",
    "```\n",
    "dat  = loadFile(\"20220128_N2A_pkg_cmv_mp1.asc\", headers= [\"index\", \"ti\", \"i\", \"tp\",\"p\",\"tv\",\"v\"])\n",
    "\n",
    "\n",
    "Now, lets see our new function in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafc9a90-6ec7-4971-8747-125c55528f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = generatePaths()\n",
    "dat  = loadFile(files[0], headers= [\"index\", \"ti\", \"i\", \"tp\",\"p\",\"tv\",\"v\"])\n",
    "datPreprocessed = preprocess(dat)\n",
    "datPreprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdf235d-246d-4637-acaa-b214cc5dde7c",
   "metadata": {},
   "source": [
    "We just accomplished in 2 lines of code what previously took 6. Functions are a great way to modularize your code. As you come up with complicated routines, if certain parts of that routine fit into neat modules you should move them into functions. It has the added benefit of making troubleshooting easier. If something goes wrong with a line of code during analysis it is much easier if you can direct your attention to fixing a single small function rather than a huge mess of scripts that aren't organized.\n",
    "\n",
    "As a note, see that we simply typed `preprocess(dat)` instead of `preprocess(df=dat)`. Python is smart enough to know that we want to assign dat to df since the function only has a single argument so this is shorthand. Even if there were multiple arguments python will try to automatically place them in the order that they are defined by the function. In most cases where there are multiple arguments you likely want to call the variables explicitly however for readability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca5b310-d42e-40a0-8550-eecf39be58af",
   "metadata": {},
   "source": [
    "# Visualize Data\n",
    "\n",
    "We will initially visualize all the data and highlight the region used for background subtraction. Check that there isn't a lot of noise here that may affect the background subtraction process. The relevant region (between 500 and 1000 ms) is highlighted in red. You should check every step of your analysis to make sure that what you're seeing makes sense. Automation is great but you need to check for errors in your analysis pipeline regularly. ESPECIALLY if you change a protocol.\n",
    "\n",
    "The below code will plot the stimulus over the response and cleans up the graphs a little bit. Currently, the plots for every sweep is overlayed. Matplotlib is powerful, flexible, and ugly to code. Why don't you take what you learned to this point and create a plotting function called `plot_sweeps` using the code below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a283a989-c812-4e99-973f-1b2f9402bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(2,1, sharex=True, gridspec_kw={'height_ratios':[1,3]}, figsize=(20,10))\n",
    "\n",
    "for k, g in datPreprocessed.groupby(\"sweep\"):\n",
    "    ax1.plot(g.ti, g.p, color=\"maroon\")\n",
    "    ax1.set_ylabel(\"Pressure (mmHg)\")\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['bottom'].set_visible(False)\n",
    "    ax1.axes.xaxis.set_visible(False)\n",
    "    ax2.plot(g.ti, g.i_blsub, linewidth=0.5, color=\"black\", alpha=k*0.2)\n",
    "    ax2.axvspan(500, 1000, alpha=0.03, color='red')\n",
    "    ax2.set_xlabel(\"Time (ms)\")\n",
    "    ax2.set_ylabel(\"Current (pA)\")\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.spines['right'].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee0d978-fcb4-4fe5-8cb4-144a6d07ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi=[4900, 5600]\n",
    "datSub = datPreprocessed.query(\"ti > @roi[0]  & ti < @roi[1]\")\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, gridspec_kw={'height_ratios':[1,3]}, figsize=(20,10))\n",
    "\n",
    "for k, g in datSub.groupby(\"sweep\"):\n",
    "    ax1.plot(g.ti, g.p, color=\"maroon\")\n",
    "    ax1.set_ylabel(\"Pressure (mmHg)\")\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['bottom'].set_visible(False)\n",
    "    ax1.axes.xaxis.set_visible(False)\n",
    "    ax2.plot(g.ti, g.i_blsub, linewidth=0.5, color=\"black\", alpha=k*0.2)\n",
    "    ax2.set_xlabel(\"Time (ms)\")\n",
    "    ax2.set_ylabel(\"Current (pA)\")\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.spines['right'].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820472f1-ec63-41c6-8456-92601bf4479c",
   "metadata": {},
   "source": [
    "# Finding Peaks\n",
    "\n",
    "Next, we want to start developing the actual analysis. For this we will build a few functions to help us. The first function we will build will be to find peaks. We discussed a few conditions that are import to finding the proper peaks. First, we need a means to find the maxima of the current trace. Second, we need a means to tell it to look for this maxima within a specific window of time to avoid, for instance, picking up the capacitive transients during the voltage change. When you are trying to think of how to code a particular analysis you should modularize how you would do it into discrete steps much like we are doing here. A good practice is to dedicate 1 line of code to a single sentence. So say what you want to do in a sentence then simply translate each sentence into a line of python code!\n",
    "\n",
    "In sentences:\n",
    "\"I need to filter the trace to only look within the window of time that the stimulus is present. Then, I want to group the data by sweep and find the time of the maximum current.\"\n",
    "\n",
    "So let's deal with filtering first. We can again turn to the pandas query. From the above graph we can see that our stimulus interval is between 5100 and 5400 ms. It is reasonable to expect our peak should be within the stimulus interval. So we can write our query as follows:\n",
    "\n",
    "```\n",
    "datStim = dfPreprocessed.query(\"ti >= 5100 & ti < 5400\")\n",
    "```\n",
    "\n",
    "Now we have a new dataframe `dfStim` that only has data within the stimululs region. We can now move on to the second sentence. \"I want to group the data by sweep and find the time of maximum current\". Well, this looks like the perfect use of split-apply-combine so lets turn to pandas chaining here. Instead of the mean this time we are interested in the max. Python has a built-in function `max()` which we can call with apply here. Since, we only care about the max current we can precede the apply function with a `.i` to only operate on the one column.\n",
    "```\n",
    "datStim.groupby(\"sweep\").i.apply(max)\n",
    "```\n",
    "\n",
    "It may however be more useful to have more information and get the whole row associated with the peak. Here, I will again use a lambda function and use the numpy function `argmax()`. This function is specifically designed to find the index of the maxima. Then, I can use this to index my dataframe using `.iloc` and extract the whole row. I also chain on the command recet_index(drop=True) for formatting reasons. Otherwise pandas will add an indexing column called `sweep` based on the groupby element. The code is as follows:\n",
    "\n",
    "\n",
    "```\n",
    "datStim.groupby(\"sweep\").apply(lambda g: g.iloc[np.argmax(g.i)]).reset_index(drop=True)\n",
    "```\n",
    "## Indexing Dataframes\n",
    "\n",
    "Similar to indexing lists and arrays you can use numbers to index dataframes. The convention is slightly different. For dataframes you should precede your square brackets with `.iloc` which is short for item location or index location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5979cf50-51fa-4cad-8d5a-8afc28815d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datStim = datPreprocessed.query(\"ti >= 5100 & ti < 5400\")\n",
    "peaks = datStim.groupby(\"sweep\").apply(lambda g: g.iloc[np.argmax(g.i)]).reset_index(drop=True)\n",
    "peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30ed9c5-6b36-4ad9-a983-ea8e02793dd3",
   "metadata": {},
   "source": [
    "## Visualize peaks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156379e3-6369-4387-8bdb-54bde0a7bf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, gridspec_kw={'height_ratios':[1,3]}, figsize=(20,10))\n",
    "\n",
    "for k, g in datSub.groupby(\"sweep\"):\n",
    "    ax1.plot(g.ti, g.p, color=\"maroon\")\n",
    "    ax1.set_ylabel(\"Pressure (mmHg)\")\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['bottom'].set_visible(False)\n",
    "    ax1.axes.xaxis.set_visible(False)\n",
    "    ax2.plot(g.ti, g.i_blsub, linewidth=0.5, color=\"black\", alpha=k*0.2)\n",
    "    ax2.set_xlabel(\"Time (ms)\")\n",
    "    ax2.set_ylabel(\"Current (pA)\")\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "    \n",
    "for k, g in peaks.groupby(\"sweep\"):\n",
    "    ax2.scatter(g.ti, g.i_blsub, facecolors='none', edgecolors='b', s=200, alpha=k*0.2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f284c2bb-cef6-4474-a3f5-d321442f2126",
   "metadata": {},
   "source": [
    "# Finding Steady State Current\n",
    "\n",
    "We are next interested in an estimate of the steady state currents. Let's take the same approach and break our problem down into sentences to approach this question. Your sentences may be something like the following:\n",
    "\n",
    "\"I need to filter the trace to only look at the last 20 ms. I then need to group the data by sweep and find the average current in this region as an estimate of steady state current.\"\n",
    "\n",
    "This is again a pretty simply 2 step process. For filtering we will again use `pd.query()`. Our stimulus actually ends closer to 5395 ms rather than 5400 so we will take this to be the end of our filter. The beginning will simply be 5395-20. We can perform this operation with the following code.\n",
    "\n",
    "```\n",
    "datPreprocessed.query(\"ti >= 5395-20 & ti < 5395\")\n",
    "```\n",
    "\n",
    "We don't need to save this to its own variable. Instead we will chain the second part of this code onto what we just did. The second sentence says I need to group the data by sweep, and find the mean current. In code this will look like the following:\n",
    "\n",
    "```\n",
    "ss = datPreprocessed.query(\"ti >= 5395-20 & ti < 5395\").groupby(\"sweep\").i.apply(np.mean)\n",
    "```\n",
    "\n",
    "Notice if you read from left to right the flow should make logical sense. First, we `.query` the data to filter the region of interest, then we `.groupby` the data to group by sweep, next we isolate the current column with `.i` since that is what we are interested in, and finally we `.apply` the `np.mean` function to find the average. The output is saved to the variable ss.\n",
    "\n",
    "```\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845a34ab-f73b-45d0-b8e9-fed615f043e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_mean = datPreprocessed.query(\"ti >= 5395-20 & ti < 5395\").groupby(\"sweep\").i.apply(np.mean)\n",
    "ss_sd = datPreprocessed.query(\"ti >= 5395-20 & ti < 5395\").groupby(\"sweep\").i.apply(np.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33175a6-915c-44d7-8ab5-db091fbc89d0",
   "metadata": {},
   "source": [
    "## Visualize Peaks and SS on the same graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc251a6d-375b-4f2d-97a5-9bbd09c06bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, gridspec_kw={'height_ratios':[1,3]}, figsize=(20,10))\n",
    "\n",
    "for k, g in datSub.groupby(\"sweep\"):\n",
    "    ax1.plot(g.ti, g.p, color=\"maroon\")\n",
    "    ax1.set_ylabel(\"Pressure (mmHg)\")\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['bottom'].set_visible(False)\n",
    "    ax1.axes.xaxis.set_visible(False)\n",
    "    ax2.plot(g.ti, g.i_blsub, linewidth=0.5, color=\"black\", alpha=k*0.2)\n",
    "    ax2.set_xlabel(\"Time (ms)\")\n",
    "    ax2.set_ylabel(\"Current (pA)\")\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "    \n",
    "for k, g in peaks.groupby(\"sweep\"):\n",
    "    ax2.scatter(g.ti, g.i_blsub, facecolors='none', edgecolors='b', s=200, alpha=k*0.2)\n",
    "\n",
    "for i, val in enumerate(ss):\n",
    "    ax2.plot([5395-20, 5395], [val, val], color = 'red', alpha=(i+1)*0.2, linewidth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1152ee27-8aee-4f56-9669-80c4d38ae60a",
   "metadata": {},
   "source": [
    "Check to make sure that your annotations are where you would expect them to be based on the raw data. If not we'll need to try and figure out what is going on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154301d8-6d7c-4055-883f-e53034f2f493",
   "metadata": {},
   "source": [
    "# Smoothing Peaks\n",
    "\n",
    "Right now we are just taking the raw maximum but this may not be the best idea as noise transients can heavily influence this value. We can reduce this by taking the average of n points around our peak. Let's start with something like n=5. In this case we will take the average of the peak along with 2 points before and 2 points after. Fortunately, instead of simply saving the peak value we saved the whole row corresponding to the peak current in the variable `peaks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9928fa32-0133-4229-a6ee-677e767aa3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58212541-f657-4f24-8470-e3ee1f44e41f",
   "metadata": {},
   "source": [
    "You can see that we have a column called `index`. With this, we know what index (or row number) corresponds to the peak current for each group. We can then just take the two rows above it, two rows below it, and average the values to get an estimate of the peak that is more robust to noisy variations. We will accomplish this using a for loop.\n",
    "\n",
    "\n",
    "## For loops\n",
    "\n",
    "There are broadly 2 types of loops in python a `while` loop where you repeat some operation while a condition holds true. In practice, you rarely use these as you run the risk of that condition never being true if you aren't careful and getting stuck in an infinite loop that can lock up your computer. Instead most operations can be accomplished with a `for` loop. Unlike `while` loops for loops have a defined endpoint. For instance, you may write something like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd8f419-2237-4ea4-a04d-319eac2cf947",
   "metadata": {},
   "source": [
    "Notice that the output it prints numbers from 0 to 4. `range()` is a convenience function that will make a list from 0 to n-1 where n, in this case, is 5. So what the for loop does is print out the value of i which will iterate over all values in the list. In essence, it is just repeating the operations in the body of the for loop 5 times. Also notice that as per python convention the line defining the `for` loop is followed by a colon and everything that is a part of the for loop after that is indented. This is the same as for defining functions and is python's way of keeping code organized and readable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda80ec7-9a57-47eb-96f4-c33fd5dc2336",
   "metadata": {},
   "source": [
    "We can use a similar principle on our data. We know that we have 5 grouups (5 sweeps) and we also know that we have 5 rows in our peaks dataframe. What we need is some what to match them up. Let's put this into works as usual.\n",
    "\n",
    "\"For every group I need to match it with the corresponding row in my variable peaks. Then I need to get the 2 rows above it and the 2 rows below it from the group. I'll average the current across the 5 rows to get my smoothed peak.\"\n",
    "\n",
    "Alright! We broke our problem down into parts. Now let's tackle each one.\n",
    "\n",
    "1. \"For every group I need to match it with the corresponding row in my variable peaks.\"\n",
    "\n",
    "First, lets save our grouped dataframe into a variable `gdf`. This is accomplised with the line `gdf = datPreprocessed.groupby('sweep')`. Next, our `for` loop will come into play. We saw that we can use `range()` to get it to repeat a fixed number of times. Remember that lists are indexed in python from 0 to n-1 exactly the same way that the `range()` function counts. Unfortunately, pandas uses a slightly different convention and will count its group numbers starting at 1. This is easy to get around though. We will start with the same loop from earlier but instead of hard coding the number 5 lets make it more flexible. Down the road we may want to average peaks across a different number of traces and we don't want our code to break. Instead of saying `range(5)`. We will let the length (number of rows) of the dataframe dictate how long the loop will be using the `len()` function. Our loop will start off looking something like as follows:\n",
    "\n",
    "```\n",
    "for i in range(len(peaks))\n",
    "    dosomething\n",
    "```\n",
    "\n",
    "This way, if our peaks dataframe had 10 rows it will run 10 times for instance. In this case, `i` is acting as our counter. So we know the first time it runs `i` is equal to 0. We also know that the first row (index 0) of peaks corresponds to the first group (index 1) of the grouped dataframe. So we can index into our dataframe by typing `peak.loc[i, 'index']` and saving the value in that location to the variable `ind`. \n",
    "\n",
    "## Indexing in Pandas\n",
    "Pandas indexing uses `.loc` or `.iloc`. If you are using column names you need to use `loc` if you are just using integer numbers you can use `iloc`.\n",
    "\n",
    "Notice here instead of using a number to index I am using the counter `i` so that the first time it goes through the loop it's pulling from row 0, the second time it pulls from row 1, etc. You can notice how powerful this can be rather than having to type the number of the row each time. It's already useful with 5 rows. Imagine if we had something like 10000 rows. \n",
    "\n",
    "## pd.get_group() to get individual groups from a grouped dataframe\n",
    "\n",
    "Similarly, we need to call the appropriate group from the grouped dataframe. For this we use the pandas command `.get_group()`. Since pandas numbers starting at 1 we cannot simply pass the counter variable `i`. As I mentioned earlier we can easily fix this. All we have to do is add 1 to the variable. So, instead of `.get_group(i)` we type `.get_group(i+1)`.\n",
    "\n",
    "Now our loop would look something like this:\n",
    "\n",
    "```\n",
    "for i in range(len(peaks)):\n",
    "    ind = peaks.loc[i,'index']\n",
    "    gdf.get_group(i+1)\n",
    "```\n",
    "\n",
    "2. \"Then I need to get the 2 rows above it and the 2 rows below it from the group.\"\n",
    "\n",
    "This sounds like a problem for `pd.query()` again since it is a matter of filtering. We have the relevant index saved in the variable ind so we can chain this onto our current loop as follow:\n",
    "\n",
    "```\n",
    "for i in range(len(peaks)):\n",
    "    ind = peaks.loc[i,'index']\n",
    "    gdf.get_group(i+1).query(\"index >= @ind - 2 & index <= @ind + 2\")\n",
    "```\n",
    "We take advantage of the fact that index is just a row counter so we can add subtract 2 and add 2 to get the relevant range.\n",
    "\n",
    "3. \"I'll average the current across the 5 rows and find the standard deviation to get my smoothed peak.\"\n",
    "\n",
    "Finally, we isolate the current column and find the mean as follows just like we have before.\n",
    "\n",
    "```\n",
    "for i in range(len(peaks)):\n",
    "    ind = peaks.loc[i,'index']\n",
    "    gdf.get_group(i+1).query(\"index >= @ind - 2 & index <= @ind + 2\").i.mean()\n",
    "```\n",
    "\n",
    "This is doing what we want it do but we currently aren't saving the results anywhere. Here, we can again take advantage of our counter. We can initialized an empty array of zeros and call it `smoothed_peaks`. Then each time we run the loop we will assign the appropriate index in that array with the value of the mean for that iteration of the `for` loop. The final code can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847f2944-bf0a-45c9-9e24-5518d8c407a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = datPreprocessed.groupby('sweep')\n",
    "\n",
    "smoothed_peaks = np.zeros(len(peaks))\n",
    "\n",
    "smoothed_peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba200c65-d346-47c8-89fe-e296a22c0b1a",
   "metadata": {},
   "source": [
    "# Visualize Smoothed Peaks\n",
    "\n",
    "I'll plot the smoothed peaks as horizontal magenta lines over the other annotations and you can see how they line up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1aed7e-abd9-49fe-923f-4633f7ef3697",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, gridspec_kw={'height_ratios':[1,3]}, figsize=(20,10))\n",
    "f.patch.set_facecolor('white')\n",
    "\n",
    "for k, g in datSub.groupby(\"sweep\"):\n",
    "    ax1.plot(g.ti, g.p, color=\"maroon\")\n",
    "    ax1.set_ylabel(\"Pressure (mmHg)\")\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['bottom'].set_visible(False)\n",
    "    ax1.axes.xaxis.set_visible(False)\n",
    "    ax2.plot(g.ti, g.i_blsub, linewidth=0.5, color=\"black\", alpha=k*0.2)\n",
    "    ax2.set_xlabel(\"Time (ms)\")\n",
    "    ax2.set_ylabel(\"Current (pA)\")\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "    \n",
    "for k, g in peaks.groupby(\"sweep\"):\n",
    "    ax2.scatter(g.ti, g.i_blsub, facecolors='none', edgecolors='b', s=200, alpha=k*0.2)\n",
    "\n",
    "for i, val in enumerate(ss):\n",
    "    ax2.plot([5395-20, 5395], [val, val], color = 'red', alpha=(i+1)*0.2, linewidth=4)\n",
    "\n",
    "for i, val in enumerate(smoothed_peaks):\n",
    "    ax2.hlines(val, xmin=5050, xmax=5395, color = 'magenta', alpha=(i+1)*0.2, linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ac677b-4d93-44ed-87ea-50d51d9f5fc6",
   "metadata": {},
   "source": [
    "# Full Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2717e306-8e23-40d2-bdeb-b1835660520b",
   "metadata": {},
   "source": [
    "Let's summarize what we have developed thus far. We have the code we need to preprocess our data by converting units and baseline subtracting, to visualize our data by plotting, to find and smooth peak estimates, and to find the mean steady-state current over a set time window. This initial development phase is often followed by a phase of restructuring our code into discrete functions. We may also want to refine some areas for improved performance and readability. This process of essentially \"cleaning up\" your workflow and modularizing it falls under the umbrella of **refactoring**. If we look at the definition of refactoring on wikipedia we see the following:\n",
    "\n",
    "> In computer programming and software design, code refactoring is the process of restructuring existing computer code—changing the factoring—without changing its external behavior. Refactoring is intended to improve the design, structure, and/or implementation of the software (its non-functional attributes), while preserving its functionality. Potential advantages of refactoring may include improved code readability and reduced complexity; these can improve the source code's maintainability and create a simpler, cleaner, or more expressive internal architecture or object model to improve extensibility. Another potential goal for refactoring is improved performance; software engineers face an ongoing challenge to write programs that perform faster or use less memory.\n",
    "\n",
    "At this time you should also document your code so that if you ever need to return to it you can quickly understand what it is doing and how it is doing it. While it is an easy step to skip over for the sake of reproducibility it is critical that you do not. The strength of python is its readability when compared to many other languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c29498-9c47-41c5-8274-53d19938d729",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = generatePaths(suffix = \"pos.asc\")\n",
    "#files = files[0:4]\n",
    "\n",
    "for file in files:\n",
    "    postraces = loadFile(file, headers= [\"index\", \"ti\", \"i\", \"tp\",\"p\",\"tv\",\"v\"])\n",
    "    filename = file.split(\"/\")[-1][0:-4]\n",
    "    print(filename)\n",
    "    pospreprocessed = preprocess(postraces)\n",
    "    posanalyzed = inactivationanalysis(pospreprocessed)\n",
    "    posanalyzed.to_csv(file[0:-4] + \"_analyzed.csv\")\n",
    "    \n",
    "    ax1, ax2 = plotSweeps(postraces, \"ti\", \"i\", p_kwargs={\"color\":\"maroon\",\"linewidth\":0.5}, i_kwargs={\"color\":\"black\",\"linewidth\":0.5})\n",
    "    plt.savefig(file[0:-4] + '_rawTraces.png', bbox_inches='tight', dpi=300, transparent=False)\n",
    "   \n",
    "    ax1, ax2 = plotSweeps(pospreprocessed, \"ti\", \"i_blsub\", p_kwargs={\"color\":\"maroon\",\"linewidth\":0.5}, i_kwargs={\"color\":\"black\",\"linewidth\":0.5})\n",
    "    ax1.set_xlim([4900,5600])\n",
    "    ax2.set_xlim([4900,5600])\n",
    "   \n",
    "    for i in range(len(posanalyzed)):\n",
    "        ax2.scatter(posanalyzed.peak_time, posanalyzed.mean_peak, facecolors='none', edgecolors='b', s=200)\n",
    "        ax2.plot([5395-20, 5395], [posanalyzed.mean_ss, posanalyzed.mean_ss], color = 'red', linewidth=1)\n",
    "   \n",
    "    plt.savefig(file[0:-4] + '_analyzedTraces.png', bbox_inches='tight', dpi=300, transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "51a29850-4e5f-45fe-a06a-a39d648fb6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzed_files = generatePaths(suffix = \"analyzed.csv\")\n",
    "analyzed_data = analyzedtable(analyzed_files)\n",
    "\n",
    "analyzed_data.to_csv(\"inactivation_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2a5bd7b1-8907-4b9e-8796-87dea2dc0193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peak_time                                    5394.8\n",
       "mean_peak                                986.748373\n",
       "sd_peak                                    6.022943\n",
       "mean_ss                                  975.112153\n",
       "sd_ss                                      8.077037\n",
       "promoter                                        CMV\n",
       "filename     20220125_N2A_mp1_CMV1_pos_analyzed.csv\n",
       "condition                                       pos\n",
       "Name: 4, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots()\n",
    "\n",
    "for k, g in analyzed_data.groupby(\"filename\"):\n",
    "    ax.scatter(g.mean_peak, g.mean_ss/g.mean_peak)\n",
    "    \n",
    "    \n",
    "    \n",
    "analyzed_data.iloc[analyzed_data.mean_peak.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cffb762-ebd5-4115-bc6f-4861dca3a4ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
